{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.4\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from datetime import datetime\n",
    "import distutils.dir_util\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "\n",
    "from keras import __version__\n",
    "print(__version__)\n",
    "\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR = \"/home/ubuntu/data/fish\"\n",
    "TEST_STG1_DIR = DATA_ROOT_DIR + '/test_stg1/'\n",
    "TEST_STG2_DIR = DATA_ROOT_DIR + '/test_stg2/'\n",
    "RESULTS_DIR = DATA_ROOT_DIR + '/results/'\n",
    "\n",
    "SAMPLE_ROOT_DIR = DATA_ROOT_DIR + \"/sample/\"\n",
    "TRAIN_SAMPLE_DIR = SAMPLE_ROOT_DIR + '/train/'\n",
    "VALID_SAMPLE_DIR = SAMPLE_ROOT_DIR + '/valid/'\n",
    "TRAIN_DIR = DATA_ROOT_DIR + '/train/'\n",
    "VALID_DIR = DATA_ROOT_DIR + '/valid/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mkdir -p $VALID_DIR\n",
    "%mkdir -p $RESULTS_DIR\n",
    "%mkdir -p $TRAIN_SAMPLE_DIR\n",
    "#%mkdir -p $VALID_SAMPLE_DIR # \"OMITTED AS VALIDATION SIZE QUITE SMALL ANYWAY\"\n",
    "%mkdir -p $TEST_STG1_DIR/unknown\n",
    "%mkdir -p $TEST_STG2_DIR/unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open sample_submission_stg1.csv.zip, sample_submission_stg1.csv.zip.zip or sample_submission_stg1.csv.zip.ZIP.\n",
      "unzip:  cannot find or open sample_submission_stg2.csv.zip, sample_submission_stg2.csv.zip.zip or sample_submission_stg2.csv.zip.ZIP.\n",
      "unzip:  cannot find or open test_stg1.zip, test_stg1.zip.zip or test_stg1.zip.ZIP.\n",
      "/bin/sh: 1: 7za: not found\n",
      "unzip:  cannot find or open train.zip, train.zip.zip or train.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!unzip sample_submission_stg1.csv.zip\n",
    "!unzip sample_submission_stg2.csv.zip\n",
    "!unzip test_stg1.zip\n",
    "!7za e test_stg2.7z\n",
    "!unzip train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/fish/train\n",
      "ALB=1492\r\n",
      "BET=172\r\n",
      "DOL=108\r\n",
      "LAG=58\r\n",
      "NoF=406\r\n",
      "OTHER=258\r\n",
      "SHARK=152\r\n",
      "YFT=630\r\n"
     ]
    }
   ],
   "source": [
    "%cd '/home/ubuntu/data/fish/train'\n",
    "!ls | while read line; do echo \"$line\"=`ls \"$line\" | wc -l`; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3777\n"
     ]
    }
   ],
   "source": [
    "print(1719+200+117+67+465+299+176+734)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/fish//train/\n",
      "/home/ubuntu/data/fish//sample//train/\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_DIR)\n",
    "print(TRAIN_SAMPLE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/fish/train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%cd /home/ubuntu/data/fish/train\n",
    "\n",
    "import glob \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "g = glob.glob('*')\n",
    "for d in g: os.mkdir('../valid/'+d)\n",
    "\n",
    "g = glob.glob('*/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(500): os.rename(shuf[i], '../valid/' + shuf[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/fish/valid\n",
      "ALB=226\r\n",
      "BET=28\r\n",
      "DOL=9\r\n",
      "LAG=9\r\n",
      "NoF=59\r\n",
      "OTHER=41\r\n",
      "SHARK=24\r\n",
      "YFT=104\r\n"
     ]
    }
   ],
   "source": [
    "%cd '/home/ubuntu/data/fish/valid'\n",
    "!ls | while read line; do echo \"$line\"=`ls \"$line\" | wc -l`; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.151474530831\n",
      "0.162790697674\n",
      "0.0833333333333\n",
      "0.155172413793\n",
      "0.145320197044\n",
      "0.158914728682\n",
      "0.157894736842\n",
      "0.165079365079\n"
     ]
    }
   ],
   "source": [
    "print float(226)/float(1492)\n",
    "print float(28)/float(172)\n",
    "print float(9)/float(108)\n",
    "print float(9)/float(58)\n",
    "print float(59)/float(406)\n",
    "print float(41)/float(258)\n",
    "print float(24)/float(152)\n",
    "print float(104)/float(630)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Lambda, BatchNormalization, Conv2D, ZeroPadding2D, MaxPooling2D, AveragePooling2D, Activation, Flatten, Dropout\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "resnet_base = ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "#classifier_input_shape = resnet_base.layers[-1].output_shape[1:] # i.e. shape of conv features (produces (None, None, None, 2048))\n",
    "classifier_input_shape = (1, 1, 2048)\n",
    "classifier_input = Input(shape=classifier_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3276 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def preprocess_input(x, data_format=None):\n",
    "    \"\"\"Preprocesses a tensor encoding a batch of images.\n",
    "    # Arguments\n",
    "        x: input Numpy tensor, 4D.\n",
    "        data_format: data format of the image tensor.\n",
    "    # Returns\n",
    "        Preprocessed tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # 'RGB'->'BGR'\n",
    "    x = x[:, :, ::-1]\n",
    "    # Zero-center by mean pixel\n",
    "    x[:, :, 0] -= 103.939\n",
    "    x[:, :, 1] -= 116.779\n",
    "    x[:, :, 2] -= 123.68\n",
    "    return x\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "#batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/ubuntu/data/fish/train',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=56,\n",
    "        #batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        '/home/ubuntu/data/fish/valid',\n",
    "        target_size=(224, 224),\n",
    "        shuffle=False,\n",
    "        #batch_size=batch_size,\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_precomputed_data(model, batches):\n",
    "    filenames = batches.filenames\n",
    "    conv_features = model.predict_generator(batches, (batches.samples/batches.batch_size ))\n",
    "    #conv_features = model.predict_generator(batches,\n",
    "     #                              len(batches.filenames))\n",
    "    labels = to_categorical(batches.classes)\n",
    "    return (filenames, conv_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_filenames, trn_conv_features, trn_labels = create_precomputed_data(resnet_base, train_generator)\n",
    "val_filenames, val_conv_features, val_labels = create_precomputed_data(resnet_base, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3276, 1, 1, 2048)\n",
      "(500, 1, 1, 2048)\n"
     ]
    }
   ],
   "source": [
    "print trn_conv_features.shape\n",
    "print val_conv_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(trn_filenames) == 3276, \"trn_filenames not as expected\"\n",
    "assert trn_conv_features.shape == (3276, 1, 1, 2048), \"trn_conv_features not as expected\"\n",
    "assert trn_labels.shape == (3276, 8), \"trn_labels not as expected\"\n",
    "\n",
    "assert len(val_filenames) == 500, \"val_filenames not as expected\"\n",
    "assert val_conv_features.shape == (500, 1, 1, 2048), \"val_conv_features not as expected\"\n",
    "assert val_labels.shape == (500, 8), \"val_labels not as expected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save precomputed convolutional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir -p $RESULTS_DIR/ResNet50_conv_feats\n",
    "\n",
    "import bcolz\n",
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "def save_precomputed_data(filenames, conv_feats, labels, features_base_name=\"ResNet50_conv_feats/trn_\"):\n",
    "    save_array(RESULTS_DIR+\"/\"+features_base_name+'filenames.dat', np.array(filenames))\n",
    "    save_array(RESULTS_DIR+\"/\"+features_base_name+'conv_feats.dat', conv_feats)\n",
    "    save_array(RESULTS_DIR+\"/\"+features_base_name+'labels.dat', np.array(labels))\n",
    "    \n",
    "save_precomputed_data(trn_filenames, trn_conv_features, trn_labels, \"ResNet50_conv_feats/trn_\")\n",
    "save_precomputed_data(val_filenames, val_conv_features, val_labels, \"ResNet50_conv_feats/val_\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load precomputed convolutional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "def load_precomputed_data(features_base_name=\"ResNet50_conv_feats/trn_\"):\n",
    "    filenames = load_array(RESULTS_DIR+\"/\"+features_base_name+'filenames.dat').tolist()\n",
    "    conv_feats = load_array(RESULTS_DIR+\"/\"+features_base_name+'conv_feats.dat')\n",
    "    labels = load_array(RESULTS_DIR+\"/\"+features_base_name+'labels.dat')\n",
    "    return filenames, conv_feats, labels\n",
    "\n",
    "trn_filenames, trn_conv_features, trn_labels = load_precomputed_data(\"ResNet50_conv_feats/trn_\")\n",
    "val_filenames, val_conv_features, val_labels = load_precomputed_data(\"ResNet50_conv_feats/val_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier model\n",
    "\n",
    "x= Flatten()(classifier_input)\n",
    "x = Dense(8, activation='softmax')(x)\n",
    "                                                     \n",
    "classifier_model_v1 = Model(classifier_input, x)\n",
    "\n",
    "#from keras.optimizers import SGD\n",
    "classifier_model_v1.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_precomputed_helper(model, result_dir_name, lr=0.1, nb_epoch=1):  \n",
    "    K.set_value(model.optimizer.lr, lr)\n",
    "    \n",
    "    now = datetime.now().strftime(\"%Y%m%d_%H%M%S.h5\")\n",
    "    results_dir = RESULTS_DIR + \"/\" + result_dir_name + \"/\"\n",
    "    distutils.dir_util.mkpath(results_dir)\n",
    "    \n",
    "    model.fit(trn_conv_features, trn_labels,\n",
    "              batch_size=32, \n",
    "              epochs=nb_epoch,\n",
    "              validation_data=(val_conv_features, val_labels),\n",
    "              shuffle=True, \n",
    "              callbacks=[CSVLogger(results_dir+\"epoch_results.csv\", separator=',', append=True)])\n",
    "    model.save_weights(results_dir + now)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3276 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "3276/3276 [==============================] - 0s - loss: 2.4111 - acc: 0.8504 - val_loss: 2.3931 - val_acc: 0.8320\n",
      "Epoch 2/10\n",
      "3276/3276 [==============================] - 0s - loss: 2.4110 - acc: 0.8504 - val_loss: 2.3746 - val_acc: 0.8300\n",
      "Epoch 3/10\n",
      "3276/3276 [==============================] - 0s - loss: 2.4110 - acc: 0.8504 - val_loss: 2.3788 - val_acc: 0.8360\n",
      "Epoch 4/10\n",
      "3276/3276 [==============================] - 0s - loss: 2.4110 - acc: 0.8504 - val_loss: 2.3755 - val_acc: 0.8360\n",
      "Epoch 5/10\n",
      "3276/3276 [==============================] - 0s - loss: 2.4110 - acc: 0.8504 - val_loss: 2.3694 - val_acc: 0.8400\n",
      "Epoch 6/10\n",
      "3276/3276 [==============================] - 0s - loss: 2.4110 - acc: 0.8504 - val_loss: 2.3810 - val_acc: 0.8360\n",
      "Epoch 7/10\n",
      "3276/3276 [==============================] - 0s - loss: 2.4110 - acc: 0.8504 - val_loss: 2.3808 - val_acc: 0.8340\n",
      "Epoch 8/10\n",
      "3276/3276 [==============================] - 0s - loss: 2.4110 - acc: 0.8504 - val_loss: 2.3780 - val_acc: 0.8320\n",
      "Epoch 9/10\n",
      "3276/3276 [==============================] - 0s - loss: 2.4110 - acc: 0.8504 - val_loss: 2.3768 - val_acc: 0.8360\n",
      "Epoch 10/10\n",
      "3276/3276 [==============================] - 0s - loss: 2.4110 - acc: 0.8504 - val_loss: 2.3739 - val_acc: 0.8340\n"
     ]
    }
   ],
   "source": [
    "classifier_model_v1 = fit_precomputed_helper(classifier_model_v1, \"classifier_model_v1\", lr=0.01, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 1, 1, 2048)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 16392     \n",
      "=================================================================\n",
      "Total params: 16,392\n",
      "Trainable params: 16,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model_v1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_7 to have shape (None, 1, 1, 2048) but got array with shape (56, 224, 224, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-68da17ce2343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         validation_steps=200)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1625\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1303\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1306\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1307\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_7 to have shape (None, 1, 1, 2048) but got array with shape (56, 224, 224, 3)"
     ]
    }
   ],
   "source": [
    "classifier_model_v1.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=200,\n",
    "        epochs=1,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
